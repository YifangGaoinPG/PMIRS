"""
This script implements the similarity computation between image and text embeddings
generated by a PMIRS model under embedding-level obfuscation.

Model Architecture:
Following the automated distillation process described in the previous subsection,
the most optimal and efficient model configuration was selected. It uses a Vision Transformer (ViT)
with 12 layers as the image encoder and a 9-layer transformer as the text encoder, both producing 512-dimensional embeddings.

Obfuscation Mechanism:
- Before similarity computation, both image and text embeddings are transformed via block-wise obfuscation.
- The 512-dimensional embedding is partitioned into four 128-dimensional blocks.
- Each block is multiplied by a unique invertible confusion matrix and injected with structured noise.
- During retrieval, noise is removed and the inverse transformation is applied to recover the embeddings.

Embedding Flow:
- Image input is resized to 224x224 and split into 49 patches.
- Patches are embedded and passed through the transformer.
- The [CLS] token is projected to form z_image.
- Text is tokenized with max 77 tokens and embedded.
- The [EOS] token is projected to form z_text.
- Obfuscation is applied to both z_image and z_text before retrieval.
- Similarity is computed after deobfuscation using cosine similarity:

  sim(z_image, z_text) = (z_image â€¢ z_text) / (||z_image|| * ||z_text||)
"""


import torch
from PIL import Image
import open_clip
import random
import os
import time
import math

total_tests_done = 0

def block_obfuscate(z, Q_blocks, noise=None):
    """Block-wise obfuscation of a 512-dim vector z using 4x128 blocks."""
    B, D = z.shape
    assert D == 512, "Embedding must be 512-dim"
    z_blocks = z.view(B, 4, 128)
    Q = torch.block_diag(*Q_blocks).to(z.device)
    if noise is None:
        noise = torch.zeros_like(z)
    return z @ Q + noise

def block_deobfuscate(z_obf, Q_blocks, noise=None):
    """Inverse of block-wise obfuscation"""
    Q = torch.block_diag(*Q_blocks).to(z_obf.device)
    Q_inv = torch.inverse(Q)
    if noise is None:
        noise = torch.zeros_like(z_obf)
    return (z_obf - noise) @ Q_inv

def generate_random_invertible_matrix(dim, seed=None):
    """
    Generate a random invertible matrix of shape (dim, dim).
    
    Args:
        dim (int): Dimension of the square matrix.
        seed (int, optional): Random seed for reproducibility.
    
    Returns:
        torch.Tensor: An invertible matrix of shape (dim, dim).
    """
    if seed is not None:
        torch.manual_seed(seed)
    
    while True:
        # Generate a random matrix
        A = torch.randn(dim, dim)
        # Check if the matrix is invertible by computing its determinant
        if torch.det(A) != 0:
            return A
def set_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def calculate_threshold(num_tests, alpha=1.0, beta=1.0):
    """
    Calculate the dynamic threshold using the formula:
    threshold = alpha / sqrt(log(num_tests + beta))
    
    Parameters:
        num_tests (int): The number of classes (labels) being tested.
        alpha (float): A scaling factor (default is 1.0).
        beta (float): An offset to prevent log(0) (default is 1.0).
    
    Returns:
        float: The dynamically calculated threshold.
    """
    return alpha / math.sqrt(math.log(num_tests + beta))

def get_image_label(image_path):
    # Extract the directory path of the image file
    dir_path = os.path.dirname(image_path)    
    # Get the last part of the directory path (class label)
    class_label = os.path.basename(dir_path)    
    return class_label

def load_labels_from_directory(directory):
    """Load labels dynamically from folder names."""
    label_list = []
    for folder_name in os.listdir(directory):
        folder_path = os.path.join(directory, folder_name)
        if os.path.isdir(folder_path):  # Ensure it's a directory
            label_list.append(folder_name)
    return label_list

def test_image_text_similarity(model, preprocess, tokenizer, image_dir, label_list, num_tests, num_iterations, threshold):
    """Test image-text similarity for a single dictionary, calculating precision, recall, F1-score, and search time."""
    global total_tests_done

    # Initialize lists to track the metrics for each iteration
    
    avg_iteration_results=[]
    device = "cuda" if torch.cuda.is_available() else "cpu"

    for _ in range(num_iterations):
        iteration_search_times = []  # Store search times for each test in this iteration
        labels_in_this_iteration = []
        labels_in_this_iteration_paths = []
        TP=FP=FN=0
        labels_in_this_iteration = random.sample(label_list, num_tests)
        print("labels_in_this_iteration: ", labels_in_this_iteration)
        labels_in_this_iteration_paths = [os.path.join(image_dir, label) for label in labels_in_this_iteration]
        #print("labels_in_this_iteration_paths", labels_in_this_iteration_paths)
        image_files = [os.path.join(root, file) for path in labels_in_this_iteration_paths for root, dirs, files in os.walk(path) for file in files if file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'))]
        random.shuffle(image_files)
        #print("image_files: ", image_files)
        print("Number of images under processing: ", len(image_files))
        random_number = random.randint(0, 9)

        precision_list = []
        recall_list = []
        f1_score_list = []
        search_times = []

        for i in range(random_number, num_tests*50, 10):
            # Randomly select an image from the image_files folder
            print("Now checking image path: ", image_files[i])

            #image_fname = random.choice(image_files)
            start_time = time.time()
            # Tokenize the true label (list of labels)
            text = tokenizer(labels_in_this_iteration).to(device)
            image_fname = image_files[i]
            # find true label
            true_label = get_image_label(image_fname)
            true_label = [true_label]
            print("The true label is: ", true_label)
            # Preprocess the image
            image = preprocess(Image.open(image_fname)).unsqueeze(0).to(device)            
            # Encode image and text features
            with torch.no_grad(), torch.cuda.amp.autocast():
                image_features = model.encode_image(image)
                text_features = model.encode_text(text)

                # Normalize features
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)

                # === Begin Obfuscation ===
                Q_blocks = [generate_random_invertible_matrix(128, seed=i) for i in range(4)]  
                noise = torch.zeros_like(image_features)

                image_obf = block_obfuscate(image_features, Q_blocks, noise)
                text_obf = block_obfuscate(text_features, Q_blocks, noise)

                image_recovered = block_deobfuscate(image_obf, Q_blocks, noise)
                text_recovered = block_deobfuscate(text_obf, Q_blocks, noise)
                
                # Calculate probabilities (similarity scores)
                text_probs = (100.0 * image_recovered @ text_recovered.T).softmax(dim=-1)

                # Get the top matches based on similarity
                similarities = text_probs.squeeze().tolist()
                print("similarities: ", similarities)

            # Find the similarity scores for each label
            similarity_scores = [(similarity, label) for similarity, label in zip(similarities, labels_in_this_iteration)]
            print("similarity_scores: ", similarity_scores)

            # Sort the labels based on their similarity score in descending order
            similarity_scores.sort(reverse=True, key=lambda x: x[0])
            print("similarity_scores sorted: ", similarity_scores)
            '''
            # Apply threshold to filter labels
            filtered_scores = [(similarity, label) for similarity, label in similarity_scores if similarity >= threshold]

            # dynamically adjust K
            K = max(1, round(num_tests * 0.2))
            predicted_labels = [label for similarity, label in filtered_scores[:K]]
            '''
            K = max(1, round(num_tests * 0.2))
            print("K and threshold: ", K, threshold)
            predicted_labels = [
                label for similarity, label in sorted(similarity_scores, reverse=True, key=lambda x: x[0])[:K]
                if similarity >= threshold]

            print("predicted_labels: ", predicted_labels)

            # Update TP and FN for multi-label classification
            for label in true_label:
                print("Label: ", label)
                print("true_label: ", true_label)
                if label in predicted_labels:
                    TP += 1  # True Positive (correct classification)
                else:
                    FN += 1  # False Negative (missed true label)

            # Update FP
            for predicted_label in predicted_labels:
                if predicted_label not in true_label:
                    FP += 1  # False Positive (irrelevant label predicted)

            print("TP FP FN in this round: ", TP, FP, FN)
            # End timer for search time
            search_time = time.time() - start_time
            iteration_search_times.append(search_time)

            total_tests_done += 1
            print(f"Total tests done so far: {total_tests_done}")

            precision = TP / (TP + FP) if (TP + FP) > 0 else 0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            print("precision, recall, f1 in this round: ", precision, recall, f1)

            # Calculate average search time for this iteration
            avg_search_time = sum(iteration_search_times) / len(iteration_search_times)
            # Record the results for each iteration
            precision_list.append(precision)
            recall_list.append(recall)
            f1_score_list.append(f1)
            search_times.append(avg_search_time)

        # Calculate average metrics over all iterations
        print(sum(precision_list))
        avg_precision = sum(precision_list) / len(precision_list)
        #avg_recall = sum(recall_list) / (num_iterations*num_tests*5)
        avg_recall = sum(recall_list)/len(recall_list)
        avg_f1_score = sum(f1_score_list) / len(f1_score_list)
        avg_search_time = sum(search_times) / len(search_times)

        # Save iteration results
        avg_iteration_results.append({
            "precision": avg_precision,
            "recall": avg_recall,
            "f1_score": avg_f1_score,
            "search_time": avg_search_time
        })



    # Format and append the results
    formatted_results = {
        "avg_precision": avg_precision,
        "avg_recall": avg_recall,
        "avg_f1_score": avg_f1_score,
        "avg_search_time": avg_search_time,
    }


    print("Final Results:")
    print(f"Avg Precision: {avg_precision:.5f}")
    print(f"Avg Recall: {avg_recall:.5f}")
    print(f"Avg F1 Score: {avg_f1_score:.5f}")
    print(f"Avg Search Time: {avg_search_time:.5f} seconds")

    # Print all iteration results after the loop
    print("\nAverage Metrics for Each Iteration:")
    for idx, iteration_result in enumerate(avg_iteration_results):
        print(f"Iteration {idx + 1}: {iteration_result}")

    return avg_iteration_results

def main():
    # Specify the path to the directory containing images
    set_seed(seed)
    image_dir = 'image_path'  # Change this to your directory path
    image_files = []
    for root, _, files in os.walk(image_dir):
        for file in files:
            image_files.append(os.path.join(root, file))

    # Load labels from the image directory
    label_dir = 'label_path'  # Change this to your label directory path
    label_list = load_labels_from_directory(label_dir)

    # Load the pre-trained PMIRS model
    arch = '12-imgL-12-textL-ViT'
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, _, preprocess = open_clip.create_model_and_transforms(arch, pretrained='LAIONYFCC400M', device=device)
    tokenizer = open_clip.get_tokenizer(arch)
    
    # Run the test to calculate precision, recall, F1-score, and search time
    num_tests = _
    threshold = calculate_threshold(num_tests, alpha=1.2, beta=0.1)
    avg_iteration_results = test_image_text_similarity(model, preprocess, tokenizer, image_dir, label_list, num_tests=num_tests, num_iterations=6, threshold= threshold)

    # Print the final results
    overall_avg_precision = sum(r["precision"] for r in avg_iteration_results) / len(avg_iteration_results)
    overall_avg_recall = sum(r["recall"] for r in avg_iteration_results) / len(avg_iteration_results)
    overall_avg_f1_score = sum(r["f1_score"] for r in avg_iteration_results) / len(avg_iteration_results)
    overall_avg_search_time = sum(r["search_time"] for r in avg_iteration_results) / len(avg_iteration_results)

    # Print overall averages
    print("\nFinal Overall Results:")
    print(f"Avg Precision: {overall_avg_precision:.5f}")
    print(f"Avg Recall: {overall_avg_recall:.5f}")
    print(f"Avg F1 Score: {overall_avg_f1_score:.5f}")
    print(f"Avg Search Time: {overall_avg_search_time:.5f} seconds")

if __name__ == "__main__":
    main()
